{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries and load data from database\n",
    "* Import Python libraries\n",
    "* Load dataset from database \n",
    "* Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk needed packages\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet',  'stopwords', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/labeled_messages_db.sqlite3')\n",
    "df = pd.read_sql_table('labeled_messages', engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "y = df.drop(['message', 'genre', 'id', 'original'], axis=1)\n",
    "category_names = list(df.columns[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization function to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    # normalize case and remove punctuation\n",
    "    remove_punc_table = str.maketrans('', '', punctuation)\n",
    "    text = text.translate(remove_punc_table).lower()\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word).lower().strip() for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(df['message'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = MultiOutputClassifier(RandomForestClassifier())\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                    ('cvect', CountVectorizer(tokenizer = tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', forest_clf )\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize a...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the model\n",
    "Report the f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_pred, y_test):\n",
    "    for i in range(len(category_names)):\n",
    "        print('Category: {}'.format(category_names[i]))\n",
    "        print(classification_report(y_test.iloc[:, i].values, y_pred[:, i]))\n",
    "        print('Accuracy: {}\\n\\n'.format(accuracy_score(y_test.iloc[:, i].values, y_pred[:, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.49      0.57      1549\n",
      "           1       0.86      0.93      0.89      5005\n",
      "\n",
      "    accuracy                           0.83      6554\n",
      "   macro avg       0.77      0.71      0.73      6554\n",
      "weighted avg       0.82      0.83      0.82      6554\n",
      "\n",
      "Accuracy: 0.8265181568507781\n",
      "\n",
      "\n",
      "Category: request\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      5438\n",
      "           1       0.81      0.49      0.61      1116\n",
      "\n",
      "    accuracy                           0.89      6554\n",
      "   macro avg       0.85      0.73      0.77      6554\n",
      "weighted avg       0.89      0.89      0.88      6554\n",
      "\n",
      "Accuracy: 0.893042416844675\n",
      "\n",
      "\n",
      "Category: offer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6520\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "Accuracy: 0.9948123283490998\n",
      "\n",
      "\n",
      "Category: aid_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82      3782\n",
      "           1       0.77      0.69      0.73      2772\n",
      "\n",
      "    accuracy                           0.78      6554\n",
      "   macro avg       0.78      0.77      0.77      6554\n",
      "weighted avg       0.78      0.78      0.78      6554\n",
      "\n",
      "Accuracy: 0.779523954836741\n",
      "\n",
      "\n",
      "Category: medical_help\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      6040\n",
      "           1       0.68      0.05      0.10       514\n",
      "\n",
      "    accuracy                           0.92      6554\n",
      "   macro avg       0.80      0.53      0.53      6554\n",
      "weighted avg       0.91      0.92      0.89      6554\n",
      "\n",
      "Accuracy: 0.9237107110161733\n",
      "\n",
      "\n",
      "Category: medical_products\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6236\n",
      "           1       0.80      0.13      0.22       318\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.88      0.56      0.60      6554\n",
      "weighted avg       0.95      0.96      0.94      6554\n",
      "\n",
      "Accuracy: 0.9560573695453158\n",
      "\n",
      "\n",
      "Category: search_and_rescue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      6364\n",
      "           1       0.60      0.06      0.11       190\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.79      0.53      0.55      6554\n",
      "weighted avg       0.96      0.97      0.96      6554\n",
      "\n",
      "Accuracy: 0.9716203844980165\n",
      "\n",
      "\n",
      "Category: security\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6421\n",
      "           1       0.50      0.01      0.01       133\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.74      0.50      0.50      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "Accuracy: 0.9797070491303022\n",
      "\n",
      "\n",
      "Category: military\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6336\n",
      "           1       0.73      0.05      0.09       218\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.85      0.52      0.54      6554\n",
      "weighted avg       0.96      0.97      0.95      6554\n",
      "\n",
      "Accuracy: 0.9678059200488252\n",
      "\n",
      "\n",
      "Category: child_alone\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6554\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       1.00      1.00      1.00      6554\n",
      "weighted avg       1.00      1.00      1.00      6554\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "\n",
      "Category: water\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6127\n",
      "           1       0.89      0.35      0.50       427\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.92      0.67      0.74      6554\n",
      "weighted avg       0.95      0.95      0.95      6554\n",
      "\n",
      "Accuracy: 0.9548367409215746\n",
      "\n",
      "\n",
      "Category: food\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      5838\n",
      "           1       0.84      0.60      0.70       716\n",
      "\n",
      "    accuracy                           0.94      6554\n",
      "   macro avg       0.90      0.80      0.84      6554\n",
      "weighted avg       0.94      0.94      0.94      6554\n",
      "\n",
      "Accuracy: 0.9440036618858713\n",
      "\n",
      "\n",
      "Category: shelter\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      5944\n",
      "           1       0.82      0.33      0.47       610\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.88      0.66      0.72      6554\n",
      "weighted avg       0.92      0.93      0.92      6554\n",
      "\n",
      "Accuracy: 0.9310344827586207\n",
      "\n",
      "\n",
      "Category: clothing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6449\n",
      "           1       0.75      0.09      0.15       105\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.87      0.54      0.57      6554\n",
      "weighted avg       0.98      0.98      0.98      6554\n",
      "\n",
      "Accuracy: 0.9848947207812023\n",
      "\n",
      "\n",
      "Category: money\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      6386\n",
      "           1       0.80      0.02      0.05       168\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.89      0.51      0.52      6554\n",
      "weighted avg       0.97      0.97      0.96      6554\n",
      "\n",
      "Accuracy: 0.9748245346353372\n",
      "\n",
      "\n",
      "Category: missing_people\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6474\n",
      "           1       1.00      0.01      0.02        80\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.99      0.51      0.51      6554\n",
      "weighted avg       0.99      0.99      0.98      6554\n",
      "\n",
      "Accuracy: 0.9879462923405554\n",
      "\n",
      "\n",
      "Category: refugees\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6341\n",
      "           1       0.17      0.00      0.01       213\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.57      0.50      0.50      6554\n",
      "weighted avg       0.94      0.97      0.95      6554\n",
      "\n",
      "Accuracy: 0.9668904485810192\n",
      "\n",
      "\n",
      "Category: death\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6240\n",
      "           1       0.80      0.18      0.29       314\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.88      0.59      0.64      6554\n",
      "weighted avg       0.95      0.96      0.95      6554\n",
      "\n",
      "Accuracy: 0.9584986267927983\n",
      "\n",
      "\n",
      "Category: other_aid\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      5674\n",
      "           1       0.61      0.03      0.06       880\n",
      "\n",
      "    accuracy                           0.87      6554\n",
      "   macro avg       0.74      0.52      0.50      6554\n",
      "weighted avg       0.83      0.87      0.81      6554\n",
      "\n",
      "Accuracy: 0.8674092157461093\n",
      "\n",
      "\n",
      "Category: infrastructure_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      6117\n",
      "           1       0.22      0.00      0.01       437\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.58      0.50      0.49      6554\n",
      "weighted avg       0.89      0.93      0.90      6554\n",
      "\n",
      "Accuracy: 0.9325602685382972\n",
      "\n",
      "\n",
      "Category: transport\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6250\n",
      "           1       0.78      0.09      0.16       304\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.87      0.55      0.57      6554\n",
      "weighted avg       0.95      0.96      0.94      6554\n",
      "\n",
      "Accuracy: 0.9566676838571865\n",
      "\n",
      "\n",
      "Category: buildings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      6171\n",
      "           1       0.80      0.11      0.19       383\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.88      0.55      0.58      6554\n",
      "weighted avg       0.94      0.95      0.93      6554\n",
      "\n",
      "Accuracy: 0.946292340555386\n",
      "\n",
      "\n",
      "Category: electricity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6422\n",
      "           1       0.62      0.04      0.07       132\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.80      0.52      0.53      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "Accuracy: 0.9801647848642051\n",
      "\n",
      "\n",
      "Category: tools\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6506\n",
      "           1       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "Accuracy: 0.9926762282575526\n",
      "\n",
      "\n",
      "Category: hospitals\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6478\n",
      "           1       0.00      0.00      0.00        76\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "Accuracy: 0.9884040280744584\n",
      "\n",
      "\n",
      "Category: shops\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6523\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      1.00      0.99      6554\n",
      "\n",
      "Accuracy: 0.9952700640830028\n",
      "\n",
      "\n",
      "Category: aid_centers\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6471\n",
      "           1       0.00      0.00      0.00        83\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.97      0.99      0.98      6554\n",
      "\n",
      "Accuracy: 0.9873359780286848\n",
      "\n",
      "\n",
      "Category: other_infrastructure\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      6251\n",
      "           1       0.20      0.00      0.01       303\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.58      0.50      0.49      6554\n",
      "weighted avg       0.92      0.95      0.93      6554\n",
      "\n",
      "Accuracy: 0.9533109551418981\n",
      "\n",
      "\n",
      "Category: weather_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92      4666\n",
      "           1       0.88      0.66      0.76      1888\n",
      "\n",
      "    accuracy                           0.88      6554\n",
      "   macro avg       0.88      0.81      0.84      6554\n",
      "weighted avg       0.88      0.88      0.87      6554\n",
      "\n",
      "Accuracy: 0.876716509002136\n",
      "\n",
      "\n",
      "Category: floods\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      6007\n",
      "           1       0.92      0.42      0.58       547\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.94      0.71      0.78      6554\n",
      "weighted avg       0.95      0.95      0.94      6554\n",
      "\n",
      "Accuracy: 0.9487335978028685\n",
      "\n",
      "\n",
      "Category: storm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      5905\n",
      "           1       0.79      0.47      0.59       649\n",
      "\n",
      "    accuracy                           0.94      6554\n",
      "   macro avg       0.87      0.73      0.78      6554\n",
      "weighted avg       0.93      0.94      0.93      6554\n",
      "\n",
      "Accuracy: 0.9353066829417149\n",
      "\n",
      "\n",
      "Category: fire\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6476\n",
      "           1       1.00      0.01      0.03        78\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.99      0.51      0.51      6554\n",
      "weighted avg       0.99      0.99      0.98      6554\n",
      "\n",
      "Accuracy: 0.9882514494964907\n",
      "\n",
      "\n",
      "Category: earthquake\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      5915\n",
      "           1       0.90      0.77      0.83       639\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.94      0.88      0.91      6554\n",
      "weighted avg       0.97      0.97      0.97      6554\n",
      "\n",
      "Accuracy: 0.9690265486725663\n",
      "\n",
      "\n",
      "Category: cold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6422\n",
      "           1       0.83      0.04      0.07       132\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.91      0.52      0.53      6554\n",
      "weighted avg       0.98      0.98      0.97      6554\n",
      "\n",
      "Accuracy: 0.9804699420201404\n",
      "\n",
      "\n",
      "Category: other_weather\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      6198\n",
      "           1       0.47      0.03      0.05       356\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.71      0.51      0.51      6554\n",
      "weighted avg       0.92      0.95      0.92      6554\n",
      "\n",
      "Accuracy: 0.9455294476655478\n",
      "\n",
      "\n",
      "Category: direct_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.92      5286\n",
      "           1       0.77      0.36      0.50      1268\n",
      "\n",
      "    accuracy                           0.86      6554\n",
      "   macro avg       0.82      0.67      0.71      6554\n",
      "weighted avg       0.85      0.86      0.83      6554\n",
      "\n",
      "Accuracy: 0.8562709795544705\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Improve the model\n",
    "Use grid search to find better parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('cvect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x000001F3882F3D38>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'cvect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x000001F3882F3D38>,\n",
       "                 vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=False),\n",
       "                       n_jobs=None),\n",
       " 'cvect__analyzer': 'word',\n",
       " 'cvect__binary': False,\n",
       " 'cvect__decode_error': 'strict',\n",
       " 'cvect__dtype': numpy.int64,\n",
       " 'cvect__encoding': 'utf-8',\n",
       " 'cvect__input': 'content',\n",
       " 'cvect__lowercase': True,\n",
       " 'cvect__max_df': 1.0,\n",
       " 'cvect__max_features': None,\n",
       " 'cvect__min_df': 1,\n",
       " 'cvect__ngram_range': (1, 1),\n",
       " 'cvect__preprocessor': None,\n",
       " 'cvect__stop_words': None,\n",
       " 'cvect__strip_accents': None,\n",
       " 'cvect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'cvect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'cvect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "     #'cvect__max_df': (0.5, 0.75, 1.0),\n",
    "     #'cvect__max_features': (None, 5000, 10000),\n",
    "     #'cvect__ngram_range': ((1, 1), (1, 2)), \n",
    "     #'tfidf__use_idf': (True, False),\n",
    "     #'tfidf__norm': ('l1', 'l2'),\n",
    "     'clf__estimator__min_samples_leaf': [2, 5, 10],\n",
    "     'clf__estimator__max_depth': [10, 50, None]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters, cv = 3, n_jobs=4, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done  27 out of  27 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u...\n",
       "                                                                                               min_weight_fraction_leaf=0.0,\n",
       "                                                                                               n_estimators=100,\n",
       "                                                                                               n_jobs=None,\n",
       "                                                                                               oob_score=False,\n",
       "                                                                                               random_state=None,\n",
       "                                                                                               verbose=0,\n",
       "                                                                                               warm_start=False),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=4,\n",
       "             param_grid={'clf__estimator__max_depth': [10, 50, None],\n",
       "                         'clf__estimator__min_samples_leaf': [2, 5, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize a...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=2,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try improving the model further.\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            \n",
    "            if not pos_tags: continue\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_v2 = Pipeline([\n",
    "        \n",
    "        ('features', FeatureUnion([\n",
    "        \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('cvect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()) )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('features', FeatureUnion(n_jobs=None,\n",
       "                transformer_list=[('text_pipeline',\n",
       "                                   Pipeline(memory=None,\n",
       "                                            steps=[('cvect',\n",
       "                                                    CountVectorizer(analyzer='word',\n",
       "                                                                    binary=False,\n",
       "                                                                    decode_error='strict',\n",
       "                                                                    dtype=<class 'numpy.int64'>,\n",
       "                                                                    encoding='utf-8',\n",
       "                                                                    input='content',\n",
       "                                                                    lowercase=True,\n",
       "                                                                    max_df=1.0,\n",
       "                                                                    max_features=None,\n",
       "                                                                    min_df=1,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 1),\n",
       "                                                                    preprocessor=None,\n",
       "                                                                    stop_words=None,\n",
       "                                                                    strip_accents=None,\n",
       "                                                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                    tokenizer=<function tokenize at 0x000001F3882F3D38>,\n",
       "                                                                    vocabulary=None)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer(norm='l2',\n",
       "                                                                     smooth_idf=True,\n",
       "                                                                     sublinear_tf=False,\n",
       "                                                                     use_idf=True))],\n",
       "                                            verbose=False)),\n",
       "                                  ('starting_verb', StartingVerbExtractor())],\n",
       "                transformer_weights=None, verbose=False)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'features': FeatureUnion(n_jobs=None,\n",
       "              transformer_list=[('text_pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('cvect',\n",
       "                                                  CountVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=1.0,\n",
       "                                                                  max_features=None,\n",
       "                                                                  min_df=1,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               1),\n",
       "                                                                  preprocessor=None,\n",
       "                                                                  stop_words=None,\n",
       "                                                                  strip_accents=None,\n",
       "                                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                  tokenizer=<function tokenize at 0x000001F3882F3D38>,\n",
       "                                                                  vocabulary=None)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer(norm='l2',\n",
       "                                                                   smooth_idf=True,\n",
       "                                                                   sublinear_tf=False,\n",
       "                                                                   use_idf=True))],\n",
       "                                          verbose=False)),\n",
       "                                ('starting_verb', StartingVerbExtractor())],\n",
       "              transformer_weights=None, verbose=False),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=False),\n",
       "                       n_jobs=None),\n",
       " 'features__n_jobs': None,\n",
       " 'features__transformer_list': [('text_pipeline',\n",
       "   Pipeline(memory=None,\n",
       "            steps=[('cvect',\n",
       "                    CountVectorizer(analyzer='word', binary=False,\n",
       "                                    decode_error='strict',\n",
       "                                    dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                    input='content', lowercase=True, max_df=1.0,\n",
       "                                    max_features=None, min_df=1,\n",
       "                                    ngram_range=(1, 1), preprocessor=None,\n",
       "                                    stop_words=None, strip_accents=None,\n",
       "                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                    tokenizer=<function tokenize at 0x000001F3882F3D38>,\n",
       "                                    vocabulary=None)),\n",
       "                   ('tfidf',\n",
       "                    TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                     sublinear_tf=False, use_idf=True))],\n",
       "            verbose=False)),\n",
       "  ('starting_verb', StartingVerbExtractor())],\n",
       " 'features__transformer_weights': None,\n",
       " 'features__verbose': False,\n",
       " 'features__text_pipeline': Pipeline(memory=None,\n",
       "          steps=[('cvect',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function tokenize at 0x000001F3882F3D38>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('tfidf',\n",
       "                  TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                   sublinear_tf=False, use_idf=True))],\n",
       "          verbose=False),\n",
       " 'features__starting_verb': StartingVerbExtractor(),\n",
       " 'features__text_pipeline__memory': None,\n",
       " 'features__text_pipeline__steps': [('cvect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x000001F3882F3D38>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))],\n",
       " 'features__text_pipeline__verbose': False,\n",
       " 'features__text_pipeline__cvect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x000001F3882F3D38>,\n",
       "                 vocabulary=None),\n",
       " 'features__text_pipeline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'features__text_pipeline__cvect__analyzer': 'word',\n",
       " 'features__text_pipeline__cvect__binary': False,\n",
       " 'features__text_pipeline__cvect__decode_error': 'strict',\n",
       " 'features__text_pipeline__cvect__dtype': numpy.int64,\n",
       " 'features__text_pipeline__cvect__encoding': 'utf-8',\n",
       " 'features__text_pipeline__cvect__input': 'content',\n",
       " 'features__text_pipeline__cvect__lowercase': True,\n",
       " 'features__text_pipeline__cvect__max_df': 1.0,\n",
       " 'features__text_pipeline__cvect__max_features': None,\n",
       " 'features__text_pipeline__cvect__min_df': 1,\n",
       " 'features__text_pipeline__cvect__ngram_range': (1, 1),\n",
       " 'features__text_pipeline__cvect__preprocessor': None,\n",
       " 'features__text_pipeline__cvect__stop_words': None,\n",
       " 'features__text_pipeline__cvect__strip_accents': None,\n",
       " 'features__text_pipeline__cvect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'features__text_pipeline__cvect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'features__text_pipeline__cvect__vocabulary': None,\n",
       " 'features__text_pipeline__tfidf__norm': 'l2',\n",
       " 'features__text_pipeline__tfidf__smooth_idf': True,\n",
       " 'features__text_pipeline__tfidf__sublinear_tf': False,\n",
       " 'features__text_pipeline__tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_v2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_v2 = {\n",
    "     'features__text_pipeline__cvect__max_df': (0.5, 0.75, 1.0),\n",
    "     #'features__text_pipeline__cvect__max_features': (None, 5000, 10000),\n",
    "     'features__text_pipeline__cvect__ngram_range': ((1, 1), (1, 2)), \n",
    "     #'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "     #'features__text_pipeline__tfidf__norm': ('l1', 'l2'),\n",
    "     'clf__estimator__min_samples_leaf': [2, 5, 10],\n",
    "     'clf__estimator__max_depth': [10, 50, None]\n",
    "}\n",
    "\n",
    "cv_v2 = GridSearchCV(pipeline_v2, parameters_v2, cv = 3, n_jobs=4, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_v2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_v2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_v2 = cv_v2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_pred_v2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export the model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(cv_v2.best_estimator_, 'classifier.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
